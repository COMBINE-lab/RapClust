rule seperate_strand_clust_with_dump:
    input:
        icl = "RapClust/sailfish/output/output.clust"
    run:
        import pandas as pd
        from collections import defaultdict

        with open("RapClust/sailfish/output/quant.sf", 'r') as f:
            data = pd.read_table(f, header=0).set_index('Name')['TPM']
        
        trCountFwd = defaultdict(float)
        trCountRev = defaultdict(float)
        f = open("RapClust/sailfish/output/strand.dump", 'r')
        line = f.readline()
        errorFlag = False
        while(line):
            #readName = line.strip().split("\t")[0]
            hits = int(f.readline().strip())
            normSum = 0
            contig2info = {}
            for i in range(hits):
                contig = f.readline().strip()
                #Since non compatible lib type can screw up the parsing
                if 'SRR' in contig:
                    errorFlag = True
                    break
                orientation = f.readline().strip()
                abundance = float(data[contig])
                normSum += abundance
                contig2info[contig] = (orientation, abundance)

            for c, (o, a) in contig2info.items():
                if (o == "ISF"):
                    trCountFwd[c] += a/normSum
                else:
                    trCountRev[c] += a/normSum
            if errorFlag == True:
                errorFlag = False
            else:
                line = f.readline()

        ifileH = open(input.icl, 'r')
        ofileH = open(input.icl+'.mod', 'w')
        for line in ifileH:
            contigsList = line.strip().split('\t')
            fStrandContigs = []
            rStrandContigs = []
            
            fhits = []
            rhits = []
            for contig in sorted(contigsList):
                fhits.append(trCountFwd[contig])
                rhits.append(trCountRev[contig])
            
            if (sum(fhits) > sum(rhits)) :
                consensus = 'F'
            else:
                consensus = 'R'

            for ind, contig in enumerate(sorted(contigsList)):
                if (fhits[ind] == rhits[ind]):
                    if consensus == 'F':
                        fStrandContigs.append(contig)
                    else:
                        rStrandContigs.append(contig)
                elif (fhits[ind] > rhits[ind]):
                    fStrandContigs.append(contig)
                else:
                    rStrandContigs.append(contig)
            
            if len(fStrandContigs) > 0:
                ofileH.write('\t'.join(fStrandContigs)+'\n')
            if len(rStrandContigs) > 0:
                ofileH.write('\t'.join(rStrandContigs)+'\n')



rule seperate_strand_clust_with_small_dump:
    input:
        icl = "RapClust/sailfish/smallDump/graph.clust",
        quant = "RapClust/sailfish/smallDump/quant.sf",
        dump = "RapClust/sailfish/smallDump/strand.dump"
    run:
        import pandas as pd
        from collections import defaultdict
        import yappi
        with open(input.quant, 'r') as f:
            data = pd.read_table(f, header=0).set_index('Name')['TPM']
        
        trCount = defaultdict(lambda: defaultdict(float))
        f = open(input.dump, 'r')
        line = f.readline()
        count = 0
        yappi.start()
        while(line):
            #Since non libtype compatable reads can screw up parsing
            if len(line) < 2:
                line = f.readline()
                continue
            spline = line.strip().split('\t')
            contigInfo = {}
            for ind in range(0, len(spline), 2):
                contig = spline[ind]
                orient = spline[ind+1]
                abundance = float(data[contig])
                contigInfo[contig] = (orient, abundance)

            normSum = sum([x for (_,x) in contigInfo.values()])

            for contig, (orient, abundance) in contigInfo.items():
                if orient == 'ISF':
                    trCount[contig]['F'] += (abundance/normSum)
                else:
                    trCount[contig]['R'] += (abundance/normSum)
            line = f.readline()
            count += 1
            if count%1000000 == 0:
                yappi.get_func_stats().print_all()
                print ("\r{} lines done".format(count), end = '')
        ifileH = open(input.icl, 'r')
        ofileH = open(input.icl+'.mod', 'w')
        for line in ifileH:
            contigsList = line.strip().split('\t')
            fStrandContigs = []
            rStrandContigs = []
            
            fhits = {}
            rhits = {}
            for contig in contigsList:
                abundance = trCount[contig]
                fhits[contig] = abundance['F']
                rhits[contig] = abundance['R']
            
            if (sum(fhits.values()) > sum(rhits.values())) :
                consensus = 'F'
            else:
                consensus = 'R'

            for contig in contigsList:
                if (fhits[contig] == rhits[contig]):
                    if consensus == 'F':
                        fStrandContigs.append(contig)
                    else:
                        rStrandContigs.append(contig)
                elif (fhits[contig] > rhits[contig]):
                    fStrandContigs.append(contig)
                else:
                    rStrandContigs.append(contig)
            
            if len(fStrandContigs) > 0:
                ofileH.write('\t'.join(fStrandContigs)+'\n')
            if len(rStrandContigs) > 0:
                ofileH.write('\t'.join(rStrandContigs)+'\n')

rule jaccard_strand_split:
    input:
        icl = "RapClust/sailfish/smallDump/graph.clust",
        contigs = "Human-Trinity/contigs.fasta"
    run:
        ocl = "RapClust/sailfish/smallDump/graph.clust.jaccard"
        kmer_size = 31

        opath = "RapClust/sailfish/smallDump/disagrementSet"
        ofile = open(opath, 'w')
        from pyfasta import Fasta
        import itertools

        ntDict = {'A':'T', 'T':'A', 'C':'G', 'G':'C'}

        ref = Fasta(contigs)
        keyMapping = {}
        for key in ref.keys():
            nkey = key.strip().split(" ")[0]
            keyMapping[nkey] = key

        icluster = open(input.icl, 'r')
        count = 0
        for cluster in icluster:
            count += 1
            contigList = cluster.strip().split('\t')
            contig2ksets = {}
            for contig in contigList:
                K = []
                Kprime = []
                seq = ref[keyMapping[contig]]
                rcseq = ''.join([ntDict[x] for x in str(seq)[::-1]])
                for i in range(len(seq)-kmer_size+1):
                    K.append(seq[i:i+31])
                    Kprime.append(rcseq[i:i+31])
                contig2ksets[contig] = {'K':K, 'Kp':Kprime}
            for comb in itertools.combinations(contigList, 2):
                c1_k = contig2ksets[comb[0]]
                c2_k = contig2ksets[comb[1]]

                disagreed = len(set(c1_k['K']).intersection(set(c2_k['Kp'])))
                agreed = len(set(c1_k['K']).intersection(set(c2_k['K'])))
                if disagreed > agreed:
                    ofile.write(comb[0] + "\t" + comb[1] + "\n")
            if count%10 == 0:
                print ("\r {} Done".format(count), end = '')
        ofile.close()

rule jaccard_strand_split_from_net:
    input:
        contigs = "Human-Trinity/contigs.fasta",
        inet = "RapClust/sailfish/smallDump/graph.net"
    run:
        import pandas as pd

        with open(input.inet, 'r') as f:
            data = pd.read_table(f, header=None)

        kmer_size = 31

        opath = "RapClust/sailfish/smallDump/graph.net.mod.fast"
        ofile = open(opath, 'w')
        from pyfasta import Fasta
        import itertools

        ntDict = {'A':'T', 'T':'A', 'C':'G', 'G':'C'}

        ref = Fasta(contigs)
        contig2kmer = {}
        count = 0
        for key in ref.keys():
            count += 1
            print ("\r {} Done".format(count), end = '')
            nkey = key.strip().split(" ")[0]
            K = []
            Kprime = []
            seq = ref[key]
            rcseq = ''.join([ntDict[x] for x in str(seq)[::-1]])
            for i in range(len(seq)-kmer_size+1):
                K.append(seq[i:i+31])
                Kprime.append(rcseq[i:i+31])
            contig2kmer[nkey] = {'K':K, 'Kp':Kprime}

        count = 0
        for i in range(len(data)):
            count += 1
            c1_k = contig2kmer[data[0][i]]
            c2_k = contig2kmer[data[1][i]]

            disagreed = len(set(c1_k['K']).intersection(set(c2_k['Kp'])))
            agreed = len(set(c1_k['K']).intersection(set(c2_k['K'])))

            if disagreed > agreed:
                data.drop([i], inplace=True)
            if count%10 == 0:
                print ("\r {} Done".format(count), end = '')

        ofile.write(data.to_csv(sep='\t', header=False, index=False))
        ofile.close()

rule larib_dump:
    input:
        contigs = "Human-Trinity/contigs.fasta",
        inet = "RapClust/sailfish/smallDump/graph.net"
    run:
        import pandas as pd

        with open(input.inet, 'r') as f:
            data = pd.read_table(f, header=None)

        kmer_size = 31

        opath = "RapClust/sailfish/smallDump/graph.net.larib"
        ofile = open(opath, 'w')
        from pyfasta import Fasta
        import itertools

        ntDict = {'A':'T', 'T':'A', 'C':'G', 'G':'C'}

        ref = Fasta(contigs)
        contig2kmer = {}
        count = 0
        for key in ref.keys():
            count += 1
            print ("\r {} Done".format(count), end = '')
            nkey = key.strip().split(" ")[0]
            K = []
            Kprime = []
            seq = ref[key]
            rcseq = ''.join([ntDict[x] for x in str(seq)[::-1]])
            for i in range(len(seq)-kmer_size+1):
                K.append(seq[i:i+31])
                Kprime.append(rcseq[i:i+31])
            contig2kmer[nkey] = {'K':K, 'Kp':Kprime}
        
        count = 0
        for i in range(len(data)):
            count += 1
            c1_k = contig2kmer[data[0][i]]
            c2_k = contig2kmer[data[1][i]]

            disagreed = len(set(c1_k['K']).intersection(set(c2_k['Kp'])))
            agreed = len(set(c1_k['K']).intersection(set(c2_k['K'])))
            
            if agreed > disagreed:
                ofile.write(str(data[0][i]) + "\t" + str(data[1][i]) + "\t" + str(data[2][i]) + "\t" + "A" + "\n")
            else:
                ofile.write(str(data[0][i]) + "\t" + str(data[1][i]) + "\t" + str(data[2][i]) + "\t" + "D" + "\n")

            if count%10 == 0:
                print ("\r {} Done".format(count), end = '')

        ofile.close()


rule net_clust_orient_intersect:
    input:
        clust = "RapClust/sailfish/smallDump/graph.clust",
        net = "RapClust/sailfish/smallDump/graph.net.larib",
        orient = "RapClust/sailfish/smallDump/graph.contig.orientation"
    run:
        import collections
        import itertools
        import pandas as pd

        contigOrient = {}
        with open(input.orient) as f:
            data = pd.read_table(f, header=None)
            for i in range(len(data)):
                contigOrient[data[0][i]] = data[1][i]
        

        with open(input.net) as f:
            data = pd.read_table(f, header=None)
            contigPairDisagreeCheck = {}
            for i in range(len(data)):
                c1 = data[0][i]
                c2 = data[1][i]
                try:
                    c1_orient = contigOrient[data[0][i]]
                    c2_orient = contigOrient[data[1][i]]
                    if c1_orient != c2_orient:
                        contigPairDisagreeCheck[tuple(sorted([c1, c2]))] = 'O'
                    else:
                        contigPairDisagreeCheck[tuple(sorted([c1, c2]))] = 'S'
                except:
                    continue

        ofile = open("RapClust/sailfish/smallDump/netClustOrientCheck", 'w')
        with open(input.clust) as f:
            for line in f:
                contigsList = line.strip().split('\t')
                count = 0
                for comb in itertools.combinations(contigsList, 2):
                    contigPair = tuple(sorted([comb[0], comb[1]]))
                    if contigPair in contigPairDisagreeCheck and contigPairDisagreeCheck[contigPair] == 'O':
                        count += 1
                ofile.write(str(len(contigsList)) +"\t" + str(count) + "\n")
        ofile.close()

rule net_clust_transcriptome_intersect:
    input:
        clust = "RapClust/sailfish/smallDump/graph.clust",
        net = "RapClust/sailfish/smallDump/graph.net.larib",
        orient = "RapClust/sailfish/smallDump/graph.contig.orientation"
    run:
        import collections
        import itertools
        import pandas as pd

        with open(input.net) as f:
            data = pd.read_table(f, header=None)
            contigPairDisagreeCheck = {}
            for i in range(len(data)):
                c1 = data[0][i]
                c2 = data[1][i]
                contigPairDisagreeCheck[tuple(sorted([c1, c2]))] = data[3][i]
        
        ofile = open("RapClust/sailfish/smallDump/netClustTrCheck", 'w')
        with open(input.clust) as f:
            for line in f:
                contigsList = line.strip().split('\t')
                count = 0
                for comb in itertools.combinations(contigsList, 2):
                    contigPair = tuple(sorted([comb[0], comb[1]]))
                    if contigPair in contigPairDisagreeCheck and contigPairDisagreeCheck[contigPair] == 'D':
                        count += 1
                ofile.write(str(len(contigsList)) +"\t" + str(count) + "\n")
        ofile.close()









