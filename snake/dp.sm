#command snakemake generate_cluster --config ifile=quant_human.clust ofile=Trinity.fasta
#generate clusters from sailfish .quant file
rule generate_clusters:
    input:
        ifile = clustFile,
        ref = contigs
    output:
        outPath = clustPath
    run:
        from pyfasta import Fasta

        refFileH = Fasta(input.ref)
        #print("Total Contigs = {}".format(len(refFileH.keys())))
        idDict = {}
        for oId in refFileH.keys():
            nId = oId.strip().split(' ')[0]
            idDict[nId] = oId
        
        ifileH = open(input.ifile,'r')
        shell("mkdir -p clusters")
        clId = 0
        #unknownContig = []
        #knownContigCount = 0
        for line in ifileH:
            contigsList = line.strip().split('\t')
            clId += 1
            knownContigs = []
            for contig in contigsList:
                if contig in idDict.keys():
                    #unknownContig.append(contig)
                    knownContigs.append(contig)

            if len(knownContigs) < 2:
                continue

            with open(output.outPath + str(clId)+'.fa', 'w') as oFile:
                for contig in knownContigs:
                    #knownContigCount += 1
                    oFile.write(">" + contig + '\n')
                    oFile.write(str(refFileH[idDict[contig]]) + '\n')
        #print ("Ignored Contigs = {}".format(len(set(unknownContig))))
        #print ("Contig Count = {}".format(knownContigCount))

rule get_one_msa:
    input:
        one_cluster
    output:
        msa
    shell:
        """
        mkdir -p {output}
        mafft --thread 25 --auto {input} > {output}msa
        """


rule get_all_msa:
    input:
        path = clustPath
    output:
        path = msaAll
    threads:
        10
    run:
        import glob
        shell("mkdir -p {}".format(output.path))
        for fileName in glob.glob(input.path + "*.fa"):
            shell("mafft --thread 10 --auto {} > {}.msa 2>/dev/null ".format(fileName, output.path+fileName.split('/')[-1]))


rule run_cpp_dp:
    input:
        one_msa
        #"./backup/test"
    shell:
        """
        clang++ --std=c++11 -O3 ./src/DP.cpp -o ./src/dp
        ./src/dp {input}
        """


rule makeBlastdb:
    input:
        cluster = clustPath
    output:
        db = "blastdatabase1/"
    run:
        shell("mkdir -p {output}")

        from pyfasta import Fasta
        import pandas as pd
        import collections
        import glob

        contigGeneMap = {}
        
        with open(contig2cuffGene, 'r') as f:
            data = pd.read_table(f, header=None, names=['contig', 'cuffgene'])
            table = data.set_index("contig").to_dict()['cuffgene']

        with open(cuffGene2refSeq, 'r') as f:
            data = pd.read_table(f, header=None, names=["cuffgene", "refseq"])
            data = data.set_index("cuffgene").to_dict()['refseq']
            for k,v in table.items():
                table[k] = data[table[k]]
        
        ccount = 0
        with open(refSeq2gene, 'r') as f:
            data = pd.read_table(f, header=None, names=["refseq", "gene"])
            data = data.set_index("refseq").to_dict()['gene']
            for k,v in table.items():
                try:
                    table[k] = data[table[k]]
                except:
                    ccount +=1
        print (ccount)
        
        logger = open("logger.txt", 'w')
        with open(transcript2gene, 'r') as f:
            data = pd.read_table(f, header=1, names = ['transcript', 'gene'])
            data =  {k: list(v) for k,v in data.groupby("gene")["transcript"]}
        f = Fasta(transcriptome)
        igCluster = 0

        for fileName in ['clusters/659.fa']:#glob.glob(input.cluster + "*.fa"):
            refFileH = Fasta(fileName)
            ambCount = 0
            fileId = fileName.strip().split("/")[-1].replace(".fa",'')

            geneList = collections.defaultdict(int)
            for line in refFileH:
                contigId = line.strip().split(' ')[0].strip()
#            print (contigId)
                if contigId in table:
                    geneList[table[contigId]] += 1
                else:
                    ambCount += 1
            
            if len(geneList) > 0:
                #print ("Ambiguous Contigs {}".format(count))
                maxfreqGene = str(max(geneList, key=geneList.get))
                logger.write(fileId + "\t"+ str(ambCount) + "\t" + maxfreqGene  +  "\n")
                print(maxfreqGene)
                with open(output.db +fileId, 'w') as wFile:
                    for transcript in data[maxfreqGene]:
                        wFile.write('>')
                        wFile.write(str(transcript))
                        wFile.write("\n"+str(f[transcript])+"\n")
            else:
                igCluster += 1


            shell("makeblastdb -in {fileName} -dbtype nucl -out {output.db}{fileId}db")
        print("ignored clusters = {}".format(igCluster))

rule run_blast:
    input:
        db = "blastdatabase/",
        query = "clusters/1.fa",
        msa = "./msa"
    output:
        "output/"
    run:
        import pandas as pd
        from pyfasta import Fasta
        import collections
    #    mkdir -p {output}
    #    clang++ --std=c++11 -O3 ./src/DP.cpp -o ./src/dp
    #    ./src/dp {input.msa} > dp.txt
        with open("dp.txt", 'r') as f:
            clusterPos = pd.read_table(f, header=0, names = ['start', 'end'])
        
        f = Fasta(input.msa)
        length = len(f[list(f.keys())[0]])
        ntCount = [{'a':0, 't':0, 'c':0, 'g':0, '-':0} for x in range(length)]

        for baseIndex in range(length):
            for contig in f:
                ntCount[baseIndex][f[contig][baseIndex]] += 1

        clusterPos = clusterPos.to_dict()
        
        #clear '-' count
        for i in ntCount:
            i['-'] = 0

        with open('consensus.fa', 'w') as f:
            for i in range(len(clusterPos['start'].keys())-1, -1, -1):
                f.write('>'+str(i+1)+'\n')
                for j in range(clusterPos['start'][i], clusterPos['end'][i]+1, 1):
                    f.write(max(ntCount[j], key=ntCount[j].get))
                f.write('\n')
             
            f.write('>'+str(0)+'\n')
            for i in range(clusterPos['end'][0]+1, length, 1):
                f.write(max(ntCount[i], key=ntCount[i].get))
            f.write('\n')

   

        shell("/usr/bin/time /usr/bin/blastn -db {input.db}db -query consensus.fa -outfmt 6 -out {output}all-vs-all1.tsv -num_threads 20")

rule run_blast_allGenes:
    input:
        db = "geneBlastDatabase/"
    run:
        import glob
        count = 0
        shell("mkdir -p outputAll")
        for database in glob.glob(input.db+"*.fq"):
            geneName = database.split('/')[-1][:-3]
            database = database + ".db"
            shell("/usr/bin/blastn -db {database} -query consensus.fa -outfmt 6 -out outputAll/{geneName}.tsv -num_threads 20")
            #shell("/usr/bin/blastn -db {database} -query consensus.fa -outfmt '7 qacc sacc evalue' -out outputAll/{geneName}.tsv -num_threads 20")
            count += 1
            print("\rDone with {}".format(count))

rule get_similarity_all:
    input:
        tsv = "output/all-vs-all1.tsv"
    run:
        from munkres import Munkres
        import pandas as pd
        import numpy as np

        with open(input.tsv, 'r') as f:
            data = pd.read_table(f, header=None, names=["clusterId", "tId", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'eval', 'i'])
            data = data.ix[:, ['clusterId', 'tId', 'eval']]

        transcripts = sorted(list(set(data['tId'])))
        clusters = sorted(list(set(data['clusterId'])))

        valDict = data.to_dict('index')
        
        matrix = np.ones((len(clusters), len(transcripts))) * 1000
        for _,v in valDict.items():
            old = matrix[clusters.index(v['clusterId'])][transcripts.index(v['tId'])]
            if old > v['eval']:
                matrix[clusters.index(v['clusterId'])][transcripts.index(v['tId'])] = v['eval']

        m = Munkres()
        indexes = m.compute(matrix)
        for row, column in indexes:
            value = matrix[row][column]
            print ('({}, {}) -> {}'.format(clusters[row], transcripts[column], value))

rule get_Transcriptome:
    input:
        "Human-Trinity/transc2gene.txt"
    output:
        path = "Transcriptome/"
    run:
        import pandas as pd
        from pyfasta import Fasta

        shell("mkdir -p {output.path}")
        with open("{}".format(input), 'r') as f:
            data = pd.read_table(f, header=0, names = ['transcript', 'gene'], index_col = 1)
        
        ref = Fasta("./Human-Trinity/Human_Genome.transcripts.fa")
        genes = (set(data.index.tolist()))
        
        unknownTransCount = 0

        for gene in genes:
            transcripts = data.loc[gene]['transcript']

            if (type(transcripts) != type('')):
                transcripts =  transcripts.tolist()
            else:
                transcripts = [transcripts]

            with open(output.path + gene.replace('/', '-') +'.gene', 'w') as f:
                for tp in transcripts:
                    if tp in ref.keys():
                        f.write(">"+tp+"\n"+str(ref[tp])+"\n")
                    else:
                        unknownTransCount += 1

        print (unknownTransCount)

rule get_graph:
    input:
        "Transcriptome/DUS1L.gene"
    output:
        "GATB/data/graph.pdf"
    shell:
        """
        ./binary/gatb-core-1.1.0-Linux/bin/dbgh5 -abundance-min 1 -in {input} -out ./GATB/data/graph.h5
        ./GATB/build/tools/GATB_1 -graph ./GATB/data/graph.h5 -out ./GATB/data/graph.dot
        dot -Tpdf GATB/data/graph.dot -o ./GATB/data/graph.pdf
        """


rule get_graph_all:
    input:
        path = "clusters/"
    output:
        "GATB/data/clgraphs/"
    run:
        import glob

        shell("mkdir -p {output}")
        count = 0
        for gene in glob.glob(input.path+"*.fa"): #['Transcriptome/DUS1L.gene']: 
                try:
                    shell("./binary/gatb-core-1.1.0-Linux/bin/dbgh5 -in {gene} -out ./GATB/data/graph.h5")
                    shell("./GATB/build/tools/GATB_1 -graph ./GATB/data/graph.h5 -out ./GATB/data/graph.dot")
                    gene = gene.split('/')[-1].replace(".gene", '')
                    shell("dot -Tpdf GATB/data/graph.dot -o ./GATB/data/graphs/{gene}.pdf")
                except:
                    count += 1
                    continue
        print (count)


